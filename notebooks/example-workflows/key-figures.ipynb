{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../thumbnail.png\" width=250 alt=\"CESM LENS image\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing Key Figures from Kay et al. (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook demonstrates how one might use the NCAR Community Earth System Model (CESM) Large Ensemble (LENS) data hosted on AWS S3. The notebook shows how to reproduce figures 2 and 4 from the [Kay et al. (2015) paper](https://doi.org/10.1175/BAMS-D-13-00255.1) describing the CESM LENS dataset.\n",
    "\n",
    "This resource is intended to be helpful for people not familiar with elements of the [Pangeo](https://pangeo.io/) framework including Jupyter Notebooks, [Xarray](https://docs.xarray.dev/en/stable/), and [Zarr](https://zarr.readthedocs.io/en/stable/) data format, or with the original paper, so it includes additional explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Intro to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Necessary | |\n",
    "| Dask | Helpful | |\n",
    "\n",
    "- **Time to learn**: estimate in minutes. For a rough idea, use 5 mins per subsection, 10 if longer; add these up for a total. Safer to round up and overestimate.\n",
    "- **System requirements**:\n",
    "    - Populate with any system, version, or non-Python software requirements if necessary\n",
    "    - Otherwise use the concepts table above and the Imports section below to describe required packages as necessary\n",
    "    - If no extra requirements, remove the **System requirements** point altogether"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import intake\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import Client\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pprint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Connect to Dask Distributed Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where you begin your first section of material, loosely tied to your objectives stated up front. Tie together your notebook as a narrative, with interspersed Markdown text, images, and more as necessary,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = sys.platform\n",
    "\n",
    "if (platform == 'win32'):\n",
    "    import multiprocessing.popen_spawn_win32\n",
    "else:\n",
    "    import multiprocessing.popen_spawn_posix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_url = 'https://ncar-cesm-lens.s3-us-west-2.amazonaws.com/catalogs/aws-cesm1-le.json'\n",
    "col = intake.open_esm_datastore(catalog_url)\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first few lines of the catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col.df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show expanded version of collection structure with details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = col.unique(columns=[\"component\", \"frequency\", \"experiment\", \"variable\"])\n",
    "pprint.pprint(uniques, compact=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data needed to construct Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search the catalog to find the desired data, in this case the reference height temperature of the atmosphere, at daily time resolution, for the Historical, 20th Century, and RCP8.5 (IPCC Representative Concentration Pathway 8.5) experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_subset = col.search(frequency=[\"daily\", \"monthly\"], component=\"atm\", variable=\"TREFHT\",\n",
    "                        experiment=[\"20C\", \"RCP85\", \"HIST\"])\n",
    "\n",
    "col_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_subset.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load catalog entries for subset into a dictionary of Xarray Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = col_subset.to_dataset_dict(zarr_kwargs={\"consolidated\": True}, storage_options={\"anon\": True})\n",
    "print(f\"\\nDataset dictionary keys:\\n {dsets.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Xarray Datasets corresponding to the three experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_HIST = dsets['atm.HIST.monthly']\n",
    "ds_20C = dsets['atm.20C.daily']\n",
    "ds_RCP85 = dsets['atm.RCP85.daily']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `dask.distributed` utility function to display size of each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed.utils import format_bytes\n",
    "print(f\"Historical: {format_bytes(ds_HIST.nbytes)}\\n\"\n",
    "      f\"20th Century: {format_bytes(ds_20C.nbytes)}\\n\"\n",
    "      f\"RCP8.5: {format_bytes(ds_RCP85.nbytes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, extract the Reference Height Temperature data variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_hist = ds_HIST[\"TREFHT\"]\n",
    "t_20c = ds_20C[\"TREFHT\"]\n",
    "t_rcp = ds_RCP85[\"TREFHT\"]\n",
    "t_20c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global surface temperature anomaly was computed relative to the 1961-90 base period in the Kay et al. paper, so extract that time slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ref = t_20c.sel(time=slice(\"1961\", \"1990\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read grid cell areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell size varies with latitude, so this must be accounted for when computing the global mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = col.search(frequency=\"static\", component=\"atm\", experiment=[\"20C\"])\n",
    "_, grid = cat.to_dataset_dict(aggregate=False, storage_options={'anon':True}, zarr_kwargs={\"consolidated\": True}).popitem()\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_area = grid.area.load()\n",
    "total_area = cell_area.sum()\n",
    "cell_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define weighted means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `resample(time=\"AS\")` does an nnnual resampling based on start of calendar year. See documentation for [Pandas resampling options](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ref_ts = (\n",
    "    (t_ref.resample(time=\"AS\").mean(\"time\") * cell_area).sum(dim=(\"lat\", \"lon\"))\n",
    "    / total_area\n",
    ").mean(dim=(\"time\", \"member_id\"))\n",
    "\n",
    "t_hist_ts = (\n",
    "    (t_hist.resample(time=\"AS\").mean(\"time\") * cell_area).sum(dim=(\"lat\", \"lon\"))\n",
    ") / total_area\n",
    "\n",
    "t_20c_ts = (\n",
    "    (t_20c.resample(time=\"AS\").mean(\"time\") * cell_area).sum(dim=(\"lat\", \"lon\"))\n",
    ") / total_area\n",
    "\n",
    "t_rcp_ts = (\n",
    "    (t_rcp.resample(time=\"AS\").mean(\"time\") * cell_area).sum(dim=(\"lat\", \"lon\"))\n",
    ") / total_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and compute means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daskâ€™s \"lazy execution\" philosophy means that until this point we have not actually read the bulk of the data. These steps take a while to complete, so we include the Notebook \"cell magic\" directive `%%time` to display elapsed and CPU times after computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_ref_mean = t_ref_ts.load()\n",
    "t_ref_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_hist_ts_df = t_hist_ts.to_series().T\n",
    "t_hist_ts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_20c_ts_df = t_20c_ts.to_series().unstack().T\n",
    "t_20c_ts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_rcp_ts_df = t_rcp_ts.to_series().unstack().T\n",
    "t_rcp_ts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get observations for Figure 2 (HadCRUT4; Morice et al. 2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observational time series data for comparison with ensemble average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsDataURL = \"https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/cru/hadcrut4/air.mon.anom.median.nc\"\n",
    "ds = xr.open_dataset(obsDataURL).load()\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_temporal_mean(ds):\n",
    "    \"\"\"\n",
    "    weight by days in each month\n",
    "    \"\"\"\n",
    "    time_bound_diff = ds.time_bnds.diff(dim=\"nbnds\")[:, 0]\n",
    "    wgts = time_bound_diff.groupby(\"time.year\") / time_bound_diff.groupby(\n",
    "        \"time.year\"\n",
    "    ).sum(xr.ALL_DIMS)\n",
    "    np.testing.assert_allclose(wgts.groupby(\"time.year\").sum(xr.ALL_DIMS), 1.0)\n",
    "    obs = ds[\"air\"]\n",
    "    cond = obs.isnull()\n",
    "    ones = xr.where(cond, 0.0, 1.0)\n",
    "    obs_sum = (obs * wgts).resample(time=\"AS\").sum(dim=\"time\")\n",
    "    ones_out = (ones * wgts).resample(time=\"AS\").sum(dim=\"time\")\n",
    "    obs_s = (obs_sum / ones_out).mean((\"lat\", \"lon\")).to_series()\n",
    "    return obs_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit observations to 20th century:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_s = weighted_temporal_mean(ds)\n",
    "obs_s = obs_s['1920':]\n",
    "obs_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ts_anom = pd.concat([t_20c_ts_df, t_rcp_ts_df]) - t_ref_mean.data\n",
    "years = [val.year for val in all_ts_anom.index]\n",
    "obs_years = [val.year for val in obs_s.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine ensemble member 1 data from historical and 20th century experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_anom = t_hist_ts_df - t_ref_mean.data\n",
    "member1 = pd.concat([hist_anom.iloc[:-2], all_ts_anom.iloc[:,0]], verify_integrity=True)\n",
    "member1_years = [val.year for val in member1.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes()\n",
    "\n",
    "ax.tick_params(right=True, top=True, direction=\"out\", length=6, width=2, grid_alpha=0.5)\n",
    "ax.plot(years, all_ts_anom.iloc[:,1:], color=\"grey\")\n",
    "ax.plot(obs_years, obs_s['1920':], color=\"red\")\n",
    "ax.plot(member1_years, member1, color=\"black\")\n",
    "\n",
    "\n",
    "ax.text(\n",
    "    0.35,\n",
    "    0.4,\n",
    "    \"observations\",\n",
    "    verticalalignment=\"bottom\",\n",
    "    horizontalalignment=\"left\",\n",
    "    transform=ax.transAxes,\n",
    "    color=\"red\",\n",
    "    fontsize=10,\n",
    ")\n",
    "ax.text(\n",
    "    0.35,\n",
    "    0.33,\n",
    "    \"members 2-40\",\n",
    "    verticalalignment=\"bottom\",\n",
    "    horizontalalignment=\"left\",\n",
    "    transform=ax.transAxes,\n",
    "    color=\"grey\",\n",
    "    fontsize=10,\n",
    ")\n",
    "ax.text(\n",
    "    0.05,\n",
    "    0.2,\n",
    "    \"member 1\",\n",
    "    verticalalignment=\"bottom\",\n",
    "    horizontalalignment=\"left\",\n",
    "    transform=ax.transAxes,\n",
    "    color=\"black\",\n",
    "    fontsize=10,\n",
    ")\n",
    "\n",
    "ax.set_xticks([1850, 1920, 1950, 2000, 2050, 2100])\n",
    "plt.ylim(-1, 5)\n",
    "plt.xlim(1850, 2100)\n",
    "plt.ylabel(\"Global Surface\\nTemperature Anomaly (K)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Add one final `---` marking the end of your body of content, and then conclude with a brief single paragraph summarizing at a high level the key pieces that were learned and how they tied to your objectives. Look to reiterate what the most important takeaways were.\n",
    "\n",
    "### What's next?\n",
    "Let Jupyter book tie this to the next (sequential) piece of content that people could move on to down below and in the sidebar. However, if this page uniquely enables your reader to tackle other nonsequential concepts throughout this book, or even external content, link to it here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources and references\n",
    "Finally, be rigorous in your citations and references as necessary. Give credit where credit is due. Also, feel free to link to relevant external material, further reading, documentation, etc. Then you're done! Give yourself a quick review, a high five, and send us a pull request. A few final notes:\n",
    " - `Kernel > Restart Kernel and Run All Cells...` to confirm that your notebook will cleanly run from start to finish\n",
    " - `Kernel > Restart Kernel and Clear All Outputs...` before committing your notebook, our machines will do the heavy lifting\n",
    " - Take credit! Provide author contact information if you'd like; if so, consider adding information here at the bottom of your notebook\n",
    " - Give credit! Attribute appropriate authorship for referenced code, information, images, etc.\n",
    " - Only include what you're legally allowed: **no copyright infringement or plagiarism**\n",
    " \n",
    "Thank you for your contribution!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
