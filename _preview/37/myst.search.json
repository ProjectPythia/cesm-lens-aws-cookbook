{"version":"1","records":[{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nThis Project Pythia Cookbook covers analysis of CESM LENS data publicly available on Amazon S3 (us-west-2 region) using Xarray and Dask","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl2":"Motivation"},"content":"The \n\nNational Center for Atmospheric Research (NCAR) Community Earth System Model Large Ensemble (\n\nCESM LENS) dataset includes a 40-member ensemble of climate simulations for the period 1920-2100. All model runs were subject to the same radiative forcing scenario: historical up to 2005, and RCP8.5 thereafter. RCP8.5 - Representative Concentration Pathway 8.5 - refers to the worst-case scenario considered in the \n\nFifth Assessment Report of the Intergovernmental Panel on Climate Change (IPCC AR5). Each of the 40 runs begins from a slightly different initial atmospheric state (created by randomly perturbing temperatures at the level of round-off error). The data comprise both surface (2D) and volumetric (3D) variables in the atmosphere, ocean, land, and ice domains.\n\nThe total LENS data volume is ~500 TB, and is traditionally accessible through the NCAR Climate Data Gateway (\n\nCDG) for download or via web services. A subset (currently ~70 TB compressed) including the most useful variables is now \n\nfreely available on AWS S3 thanks to the \n\nAWS Public Dataset Program.","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl2":"Authors"},"content":"See contributors to the NCAR/cesm-lens-aws repository","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl2":"Structure"},"content":"","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl3":"Foundations","lvl2":"Structure"},"type":"lvl3","url":"/#foundations","position":10},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl3":"Foundations","lvl2":"Structure"},"content":"There is one notebook in this section that describes how to access the CESM LENS data from AWS using Intake ESM. It includes examples of using an enhanced catalog.","type":"content","url":"/#foundations","position":11},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl3":"Example workflows","lvl2":"Structure"},"type":"lvl3","url":"/#example-workflows","position":12},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl3":"Example workflows","lvl2":"Structure"},"content":"This section contains an example of using this dataset to recreate two plots from a paper published in BAMS.","type":"content","url":"/#example-workflows","position":13},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":14},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":15},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":16},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.","type":"content","url":"/#running-on-binder","position":17},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":18},{"hierarchy":{"lvl1":"CESM LENS on AWS Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\n(Replace “cookbook-example” with the title of your cookbooks)\n\nClone the https://github.com/ProjectPythia/cesm-lens-aws-cookbook repository: git clone https://github.com/ProjectPythia/cesm-lens-aws-cookbook.git\n\nMove into the cesm-lens-aws-cookbook directorycd cesm-lens-aws-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate cla-cookbook-dev\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":19},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)"},"type":"lvl1","url":"/notebooks/example-workflows/key-figures","position":0},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/key-figures","position":1},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/example-workflows/key-figures#overview","position":2},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Overview"},"content":"This notebook demonstrates how one might use the NCAR Community Earth System Model (CESM) Large Ensemble (LENS) data hosted on AWS S3. The notebook shows how to reproduce figures 2 and 4 from the \n\nKay et al. (2015) paper describing the CESM LENS dataset \n\nKay et al., 2015.\n\nThis resource is intended to be helpful for people not familiar with elements of the \n\nPangeo framework including Jupyter Notebooks, \n\nXarray, and \n\nZarr data format, or with the original paper, so it includes additional explanation.\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#overview","position":3},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/example-workflows/key-figures#prerequisites","position":4},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\n\n\nDask\n\nHelpful\n\n\n\nTime to learn: 30 minutes\n\n\n\nNOTE: In this notebook, we access very large cloud-served datasets and use \n\nDask to parallelize our workflow. The end-to-end execution time may be on the order of an hour or more, depending on your computing resources.\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#prerequisites","position":5},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/example-workflows/key-figures#imports","position":6},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Imports"},"content":"\n\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport intake\nimport matplotlib.pyplot as plt\nfrom dask.distributed import Client\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport cmaps  # for NCL colormaps\nimport cartopy.crs as ccrs\nimport dask\nimport s3fs\n\ndask.config.set({\"distributed.scheduler.worker-saturation\": 1.0})\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#imports","position":7},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Create and Connect to Dask Distributed Cluster"},"type":"lvl2","url":"/notebooks/example-workflows/key-figures#create-and-connect-to-dask-distributed-cluster","position":8},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Create and Connect to Dask Distributed Cluster"},"content":"\n\nHere we’ll use a dask cluster to parallelize our analysis.\n\nplatform = sys.platform\n\nif (platform == 'win32'):\n    import multiprocessing.popen_spawn_win32\nelse:\n    import multiprocessing.popen_spawn_posix\n\nclient = Client()\nclient\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#create-and-connect-to-dask-distributed-cluster","position":9},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Load and Prepare Data"},"type":"lvl2","url":"/notebooks/example-workflows/key-figures#load-and-prepare-data","position":10},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Load and Prepare Data"},"content":"\n\ncatalog_url = 'https://ncar-cesm-lens.s3-us-west-2.amazonaws.com/catalogs/aws-cesm1-le.json'\ncol = intake.open_esm_datastore(catalog_url)\ncol\n\nShow the first few lines of the catalog:\n\ncol.df.head(10)\n\nShow expanded version of collection structure with details:\n\ncol.keys_info().head()\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#load-and-prepare-data","position":11},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Extract data needed to construct Figure 2","lvl2":"Load and Prepare Data"},"type":"lvl3","url":"/notebooks/example-workflows/key-figures#extract-data-needed-to-construct-figure-2","position":12},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Extract data needed to construct Figure 2","lvl2":"Load and Prepare Data"},"content":"\n\nSearch the catalog to find the desired data, in this case the reference height temperature of the atmosphere, at daily time resolution, for the Historical, 20th Century, and RCP8.5 (IPCC Representative Concentration Pathway 8.5) experiments.\n\ncol_subset = col.search(frequency=[\"daily\", \"monthly\"], component=\"atm\", variable=\"TREFHT\",\n                        experiment=[\"20C\", \"RCP85\", \"HIST\"])\n\ncol_subset\n\ncol_subset.df\n\nLoad catalog entries for subset into a dictionary of Xarray Datasets:\n\ndsets = col_subset.to_dataset_dict(zarr_kwargs={\"consolidated\": True}, storage_options={\"anon\": True})\nprint(f\"\\nDataset dictionary keys:\\n {dsets.keys()}\")\n\nDefine Xarray Datasets corresponding to the three experiments:\n\nds_HIST = dsets['atm.HIST.monthly']\nds_20C = dsets['atm.20C.daily']\nds_RCP85 = dsets['atm.RCP85.daily']\n\nUse the dask.distributed utility function to display size of each dataset:\n\nfrom dask.utils import format_bytes\nprint(f\"Historical: {format_bytes(ds_HIST.nbytes)}\\n\"\n      f\"20th Century: {format_bytes(ds_20C.nbytes)}\\n\"\n      f\"RCP8.5: {format_bytes(ds_RCP85.nbytes)}\")\n\nNow, extract the Reference Height Temperature data variable:\n\nt_hist = ds_HIST[\"TREFHT\"]\nt_20c = ds_20C[\"TREFHT\"]\nt_rcp = ds_RCP85[\"TREFHT\"]\nt_20c\n\nThe global surface temperature anomaly was computed relative to the 1961-90 base period in the Kay et al. paper, so extract that time slice:\n\nt_ref = t_20c.sel(time=slice(\"1961\", \"1990\"))\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#extract-data-needed-to-construct-figure-2","position":13},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Figure 2"},"type":"lvl2","url":"/notebooks/example-workflows/key-figures#figure-2","position":14},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Figure 2"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#figure-2","position":15},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Read grid cell areas","lvl2":"Figure 2"},"type":"lvl3","url":"/notebooks/example-workflows/key-figures#read-grid-cell-areas","position":16},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Read grid cell areas","lvl2":"Figure 2"},"content":"\n\nCell size varies with latitude, so this must be accounted for when computing the global mean.\n\ncat = col.search(frequency=\"static\", component=\"atm\", experiment=[\"20C\"])\n_, grid = cat.to_dataset_dict(aggregate=False, storage_options={'anon':True}, zarr_kwargs={\"consolidated\": True}).popitem()\ngrid\n\ncell_area = grid.area.load()\ntotal_area = cell_area.sum()\ncell_area\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#read-grid-cell-areas","position":17},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Define weighted means","lvl2":"Figure 2"},"type":"lvl3","url":"/notebooks/example-workflows/key-figures#define-weighted-means","position":18},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Define weighted means","lvl2":"Figure 2"},"content":"\n\nNote: resample(time=\"AS\") does an annual resampling based on start of calendar year. See documentation for \n\nPandas resampling options.\n\nt_ref_ts = (\n    (t_ref.resample(time=\"AS\").mean(\"time\") * cell_area).sum(dim=(\"lat\", \"lon\"))\n    / total_area\n).mean(dim=(\"time\", \"member_id\"))\n\nt_hist_ts = (\n    (t_hist.resample(time=\"AS\").mean(\"time\") * cell_area).sum(dim=(\"lat\", \"lon\"))\n) / total_area\n\nt_20c_ts = (\n    (t_20c.resample(time=\"AS\").mean(\"time\") * cell_area).sum(dim=(\"lat\", \"lon\"))\n) / total_area\n\nt_rcp_ts = (\n    (t_rcp.resample(time=\"AS\").mean(\"time\") * cell_area).sum(dim=(\"lat\", \"lon\"))\n) / total_area\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#define-weighted-means","position":19},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Read data and compute means","lvl2":"Figure 2"},"type":"lvl3","url":"/notebooks/example-workflows/key-figures#read-data-and-compute-means","position":20},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Read data and compute means","lvl2":"Figure 2"},"content":"\n\nDask’s “lazy execution” philosophy means that until this point we have not actually read the bulk of the data. Steps 1, 3, and 4 take a while to complete, so we include the Notebook “cell magic” directive %%time to display elapsed and CPU times after computation.\n\nStep 1 (takes a while)\n\n%%time\n# this cell takes a while, be patient\nt_ref_mean = t_ref_ts.load()\nt_ref_mean\n\nStep 2 (executes quickly)\n\n%%time \nt_hist_ts_df = t_hist_ts.to_series().T\n#t_hist_ts_df.head()\n\nStep 3 (takes even longer than Step 1)\n\n%%time\nt_20c_ts_df = t_20c_ts.to_series().unstack().T\nt_20c_ts_df.head()\n\nStep 4 (similar to Step 3 in its execution time)\n\n%%time\n# This also takes a while\nt_rcp_ts_df = t_rcp_ts.to_series().unstack().T\nt_rcp_ts_df.head()\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#read-data-and-compute-means","position":21},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Get observations for Figure 2 (HadCRUT4)","lvl2":"Figure 2"},"type":"lvl3","url":"/notebooks/example-workflows/key-figures#get-observations-for-figure-2-hadcrut4","position":22},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Get observations for Figure 2 (HadCRUT4)","lvl2":"Figure 2"},"content":"The HadCRUT4 temperature dataset is described by \n\nMorice et al. (2012).\n\nObservational time series data for comparison with ensemble average:\n\nobsDataURL = \"https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/cru/hadcrut4/air.mon.anom.median.nc\"\nds = xr.open_dataset(obsDataURL).load()\nds\n\ndef weighted_temporal_mean(ds):\n    \"\"\"\n    weight by days in each month\n    \"\"\"\n    time_bound_diff = ds.time_bnds.diff(dim=\"nbnds\")[:, 0]\n    wgts = time_bound_diff.groupby(\"time.year\") / time_bound_diff.groupby(\n        \"time.year\"\n    ).sum(xr.ALL_DIMS)\n    obs = ds[\"air\"]\n    cond = obs.isnull()\n    ones = xr.where(cond, 0.0, 1.0)\n    obs_sum = (obs * wgts).resample(time=\"AS\").sum(dim=\"time\")\n    ones_out = (ones * wgts).resample(time=\"AS\").sum(dim=\"time\")\n    obs_s = (obs_sum / ones_out).mean((\"lat\", \"lon\")).to_series()\n    return obs_s\n\nLimit observations to 20th century:\n\nobs_s = weighted_temporal_mean(ds)\nobs_s = obs_s['1920':]\nobs_s.head()\n\nall_ts_anom = pd.concat([t_20c_ts_df, t_rcp_ts_df]) - t_ref_mean.data\nyears = [val.year for val in all_ts_anom.index]\nobs_years = [val.year for val in obs_s.index]\n\nCombine ensemble member 1 data from historical and 20th century experiments:\n\nhist_anom = t_hist_ts_df - t_ref_mean.data\nmember1 = pd.concat([hist_anom.iloc[:-2], all_ts_anom.iloc[:,0]], verify_integrity=True)\nmember1_years = [val.year for val in member1.index]\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#get-observations-for-figure-2-hadcrut4","position":23},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Plotting Figure 2","lvl2":"Figure 2"},"type":"lvl3","url":"/notebooks/example-workflows/key-figures#plotting-figure-2","position":24},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Plotting Figure 2","lvl2":"Figure 2"},"content":"\n\nGlobal surface temperature anomaly (1961-90 base period) for individual ensemble members, and observations:\n\nax = plt.axes()\n\nax.tick_params(right=True, top=True, direction=\"out\", length=6, width=2, grid_alpha=0.5)\nax.plot(years, all_ts_anom.iloc[:,1:], color=\"grey\")\nax.plot(obs_years, obs_s['1920':], color=\"red\")\nax.plot(member1_years, member1, color=\"black\")\n\n\nax.text(\n    0.35,\n    0.4,\n    \"observations\",\n    verticalalignment=\"bottom\",\n    horizontalalignment=\"left\",\n    transform=ax.transAxes,\n    color=\"red\",\n    fontsize=10,\n)\nax.text(\n    0.35,\n    0.33,\n    \"members 2-40\",\n    verticalalignment=\"bottom\",\n    horizontalalignment=\"left\",\n    transform=ax.transAxes,\n    color=\"grey\",\n    fontsize=10,\n)\nax.text(\n    0.05,\n    0.2,\n    \"member 1\",\n    verticalalignment=\"bottom\",\n    horizontalalignment=\"left\",\n    transform=ax.transAxes,\n    color=\"black\",\n    fontsize=10,\n)\n\nax.set_xticks([1850, 1920, 1950, 2000, 2050, 2100])\nplt.ylim(-1, 5)\nplt.xlim(1850, 2100)\nplt.ylabel(\"Global Surface\\nTemperature Anomaly (K)\")\nplt.show()\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#plotting-figure-2","position":25},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Figure 4"},"type":"lvl2","url":"/notebooks/example-workflows/key-figures#figure-4","position":26},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Figure 4"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#figure-4","position":27},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Compute linear trend for winter seasons","lvl2":"Figure 4"},"type":"lvl3","url":"/notebooks/example-workflows/key-figures#compute-linear-trend-for-winter-seasons","position":28},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Compute linear trend for winter seasons","lvl2":"Figure 4"},"content":"\n\ndef linear_trend(da, dim=\"time\"):\n    da_chunk = da.chunk({dim: -1})\n    trend = xr.apply_ufunc(\n        calc_slope,\n        da_chunk,\n        vectorize=True,\n        input_core_dims=[[dim]],\n        output_core_dims=[[]],\n        output_dtypes=[np.float64],\n        dask=\"parallelized\",\n    )\n    return trend\n\n\ndef calc_slope(y):\n    \"\"\"ufunc to be used by linear_trend\"\"\"\n    x = np.arange(len(y))\n\n    # drop missing values (NaNs) from x and y\n    finite_indexes = ~np.isnan(y)\n    slope = np.nan if (np.sum(finite_indexes) < 2) else np.polyfit(x[finite_indexes], y[finite_indexes], 1)[0]\n    return slope\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#compute-linear-trend-for-winter-seasons","position":29},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Compute ensemble trends","lvl2":"Figure 4"},"type":"lvl3","url":"/notebooks/example-workflows/key-figures#compute-ensemble-trends","position":30},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Compute ensemble trends","lvl2":"Figure 4"},"content":"\n\n%%time \n# Takes several minutes\nt = xr.concat([t_20c, t_rcp], dim=\"time\")\nseasons = t.sel(time=slice(\"1979\", \"2012\")).resample(time=\"QS-DEC\").mean(\"time\")\n# Include only full seasons from 1979 and 2012\nseasons = seasons.sel(time=slice(\"1979\", \"2012\")).load()\n\nwinter_seasons = seasons.sel(\n    time=seasons.time.where(seasons.time.dt.month == 12, drop=True)\n)\nwinter_trends = linear_trend(\n    winter_seasons.chunk({\"lat\": 20, \"lon\": 20, \"time\": -1})\n).load() * len(winter_seasons.time)\n\n# Compute ensemble mean from the first 30 members\nwinter_trends_mean = winter_trends.isel(member_id=range(30)).mean(dim='member_id')\n\nMake sure that we have 34 seasons:\n\nassert len(winter_seasons.time) == 34\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#compute-ensemble-trends","position":31},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Get observations for Figure 4 (NASA GISS GisTemp)","lvl2":"Figure 4"},"type":"lvl3","url":"/notebooks/example-workflows/key-figures#get-observations-for-figure-4-nasa-giss-gistemp","position":32},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Get observations for Figure 4 (NASA GISS GisTemp)","lvl2":"Figure 4"},"content":"\n\nThis is observational time series data for comparison with ensemble average. Here we are using the \n\nGISS Surface Temperature Analysis (GISTEMP v4) from NASA’s Goddard Institute of Space Studies \n\nLenssen et al., 2019.\n\nNote\n\nWe will point to Project Pythia’s \n\nJetstream2 object-store copy of the original time series dataset, in Zarr format.\n\nThe dataset was obtained from \n\nNASA’s GISTEMP website in May 2024.\n\nDefine the URL to Project Pythia’s Jetstream2 Object Store and the path to the Zarr file.\n\nURL = 'https://js2.jetstream-cloud.org:8001'\nfilePath = 's3://pythia/gistemp1200_GHCNv4_ERSSTv5.zarr'\n\nCreate a container for the S3 file system\n\nfs = s3fs.S3FileSystem(anon=True, client_kwargs=dict(endpoint_url=URL))\n\nLink to the Zarr file as it exists on the S3 object store\n\nstore = s3fs.S3Map(root=filePath, s3=fs, check=False )\n\nds = xr.open_zarr(store, consolidated=True, chunks=\"auto\")\nds\n\nCreate an Xarray Dataset from the Zarr object\n\nRemap longitude range from [-180, 180] to [0, 360] for plotting purposes:\n\nds = ds.assign_coords(lon=((ds.lon + 360) % 360)).sortby('lon')\nds\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#get-observations-for-figure-4-nasa-giss-gistemp","position":33},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Compute observed trends","lvl2":"Figure 4"},"type":"lvl3","url":"/notebooks/example-workflows/key-figures#compute-observed-trends","position":34},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Compute observed trends","lvl2":"Figure 4"},"content":"\n\nInclude only full seasons from 1979 through 2012:\n\nobs_seasons = ds.sel(time=slice(\"1979\", \"2012\")).resample(time=\"QS-DEC\").mean(\"time\")\nobs_seasons = obs_seasons.sel(time=slice(\"1979\", \"2012\")).load()\nobs_winter_seasons = obs_seasons.sel(\n    time=obs_seasons.time.where(obs_seasons.time.dt.month == 12, drop=True)\n)\nobs_winter_seasons\n\nAnd compute observed winter trends:\n\nobs_winter_trends = linear_trend(\n    obs_winter_seasons.chunk({\"lat\": 20, \"lon\": 20, \"time\": -1})\n).load() * len(obs_winter_seasons.time)\nobs_winter_trends\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#compute-observed-trends","position":35},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Plotting Figure 4","lvl2":"Figure 4"},"type":"lvl3","url":"/notebooks/example-workflows/key-figures#plotting-figure-4","position":36},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"Plotting Figure 4","lvl2":"Figure 4"},"content":"\n\nGlobal maps of historical (1979 - 2012) boreal winter (DJF) surface air trends:\n\ncontour_levels = [-6, -5, -4, -3, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 3, 4, 5, 6]\ncolor_map = cmaps.ncl_default\n\ndef make_map_plot(nplot_rows, nplot_cols, plot_index, data, plot_label):\n    \"\"\" Create a single map subplot. \"\"\"\n    ax = plt.subplot(nplot_rows, nplot_cols, plot_index, projection = ccrs.Robinson(central_longitude = 180))\n    cplot = plt.contourf(lons, lats, data,\n                         levels = contour_levels,\n                         cmap = color_map,\n                         extend = 'both',\n                         transform = ccrs.PlateCarree())\n    ax.coastlines(color = 'grey')\n    ax.text(0.01, 0.01, plot_label, fontsize = 14, transform = ax.transAxes)\n    return cplot, ax\n\n%%time\n# Generate plot (may take a while as many individual maps are generated)\nnumPlotRows = 8\nnumPlotCols = 4\nfigWidth = 20\nfigHeight = 30\n\nfig, axs = plt.subplots(numPlotRows, numPlotCols, figsize=(figWidth,figHeight))\n\nlats = winter_trends.lat\nlons = winter_trends.lon\n\n# Create ensemble member plots\nfor ensemble_index in range(30):\n    plot_data = winter_trends.isel(member_id = ensemble_index)\n    plot_index = ensemble_index + 1\n    plot_label = str(plot_index)\n    plotRow = ensemble_index // numPlotCols\n    plotCol = ensemble_index % numPlotCols\n    # Retain axes objects for figure colorbar\n    cplot, axs[plotRow, plotCol] = make_map_plot(numPlotRows, numPlotCols, plot_index, plot_data, plot_label)\n\n# Create plots for the ensemble mean, observations, and a figure color bar.\ncplot, axs[7,2] = make_map_plot(numPlotRows, numPlotCols, 31, winter_trends_mean, 'EM')\n\nlats = obs_winter_trends.lat\nlons = obs_winter_trends.lon\ncplot, axs[7,3] = make_map_plot(numPlotRows, numPlotCols, 32, obs_winter_trends.tempanomaly, 'OBS')\n\ncbar = fig.colorbar(cplot, ax=axs, orientation='horizontal', shrink = 0.7, pad = 0.02)\ncbar.ax.set_title('1979-2012 DJF surface air temperature trends (K/34 years)', fontsize = 16)\ncbar.set_ticks(contour_levels)\ncbar.set_ticklabels(contour_levels)\n\nClose our client:\n\nclient.close()\n\n\n\n","type":"content","url":"/notebooks/example-workflows/key-figures#plotting-figure-4","position":37},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/example-workflows/key-figures#summary","position":38},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl2":"Summary"},"content":"In this notebook, we used CESM LENS data hosted on AWS to recreate two key figures in the paper that describes the project.","type":"content","url":"/notebooks/example-workflows/key-figures#summary","position":39},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/example-workflows/key-figures#whats-next","position":40},{"hierarchy":{"lvl1":"Reproducing Key Figures from Kay et al. (2015)","lvl3":"What’s next?","lvl2":"Summary"},"content":"More example workflows using these datasets may be added in the future.","type":"content","url":"/notebooks/example-workflows/key-figures#whats-next","position":41},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo"},"type":"lvl1","url":"/notebooks/foundations/enhanced-catalog","position":0},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo"},"content":"\n\n\n\n","type":"content","url":"/notebooks/foundations/enhanced-catalog","position":1},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/foundations/enhanced-catalog#overview","position":2},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Overview"},"content":"This notebook compares one \n\nIntake-ESM catalog with an enhanced version that includes additional attributes. Both catalogs are an inventory of the \n\nNCAR Community Earth System Model (CESM) Large Ensemble (LENS) data hosted on AWS S3.\n\n","type":"content","url":"/notebooks/foundations/enhanced-catalog#overview","position":3},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/foundations/enhanced-catalog#prerequisites","position":4},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Pandas\n\nNecessary\n\n\n\nTime to learn: 10 minutes\n\n\n\n","type":"content","url":"/notebooks/foundations/enhanced-catalog#prerequisites","position":5},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/foundations/enhanced-catalog#imports","position":6},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Imports"},"content":"\n\nimport intake\nimport pandas as pd\n\n","type":"content","url":"/notebooks/foundations/enhanced-catalog#imports","position":7},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Original Intake-ESM Catalog"},"type":"lvl2","url":"/notebooks/foundations/enhanced-catalog#original-intake-esm-catalog","position":8},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Original Intake-ESM Catalog"},"content":"\n\nAt import time, the intake-esm plugin is available in intake’s registry as esm_datastore and can be accessed with intake.open_esm_datastore() function.\n\ncat_url_orig = 'https://ncar-cesm-lens.s3-us-west-2.amazonaws.com/catalogs/aws-cesm1-le.json'\ncoll_orig = intake.open_esm_datastore(cat_url_orig)\ncoll_orig\n\nHere’s a summary representation:\n\nprint(coll_orig)\n\nIn an Intake-ESM catalog object, the esmcat class provides many useful attributes and functions. For example, we can get the collection’s description:\n\ncoll_orig.esmcat.description\n\nWe can also get the URL pointing to the catalog’s underlying tabular representation:\n\ncoll_orig.esmcat.catalog_file\n\nThat’s a CSV file ... let’s take a peek.\n\ndf_orig = pd.read_csv(coll_orig.esmcat.catalog_file)\ndf_orig\n\nHowever, we can save a step since an ESM catalog object provides a df instance which returns a dataframe too:\n\ndf_orig = coll_orig.df\ndf_orig\n\nPrint out a sorted list of the unique values of selected columns\n\nfor col in ['component', 'frequency', 'experiment', 'variable']:\n    unique_vals = coll_orig.unique()[col]\n    unique_vals.sort()\n    count = len(unique_vals)\n    print (col + ': ' ,unique_vals, \" count: \", count, '\\n')\n\n","type":"content","url":"/notebooks/foundations/enhanced-catalog#original-intake-esm-catalog","position":9},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Finding Data"},"type":"lvl2","url":"/notebooks/foundations/enhanced-catalog#finding-data","position":10},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Finding Data"},"content":"\n\nIf you happen to know the meaning of the variable names, you can find what data are available for that variable. For example:\n\ndf = coll_orig.search(variable='FLNS').df\ndf\n\nWe can narrow the filter to specific frequency and experiment:\n\ndf = coll_orig.search(variable='FLNS', frequency='daily', experiment='RCP85').df\ndf\n\nThe problem: Do all potential users know that `FLNS` is a CESM-specific abbreviation for \"net longwave flux at surface”? How would a novice user find out, other than by finding separate documentation, or by opening a Zarr store in the hopes that the long name might be recorded there? How do we address the fact that every climate model code seems to have a different, non-standard name for all the variables, thus making multi-source research needlessly difficult?\n\nThe solution:","type":"content","url":"/notebooks/foundations/enhanced-catalog#finding-data","position":11},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Enhanced Intake-ESM Catalog!"},"type":"lvl2","url":"/notebooks/foundations/enhanced-catalog#enhanced-intake-esm-catalog","position":12},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Enhanced Intake-ESM Catalog!"},"content":"\n\nBy adding additional columns to the Intake-ESM catalog, we should be able to improve semantic interoperability and provide potentially useful information to the users. Let’s now open the enhanced collection description file:\n\nNote: The URL for the enhanced catalog differs from the original only in that it has -enhanced appended to aws-cesm1-le\n\ncat_url = 'https://ncar-cesm-lens.s3-us-west-2.amazonaws.com/catalogs/aws-cesm1-le-enhanced.json'\ncoll = intake.open_esm_datastore(cat_url)\ncoll\n\nAs we did for the first catalog, let’s obtain the description and catalog_file attributes.\n\nprint(coll.esmcat.description) # Description of collection\nprint(\"Catalog file:\", coll.esmcat.catalog_file)\nprint(coll) # Summary of collection structure\n\n","type":"content","url":"/notebooks/foundations/enhanced-catalog#enhanced-intake-esm-catalog","position":13},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl3":"Long names","lvl2":"Enhanced Intake-ESM Catalog!"},"type":"lvl3","url":"/notebooks/foundations/enhanced-catalog#long-names","position":14},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl3":"Long names","lvl2":"Enhanced Intake-ESM Catalog!"},"content":"\n\nIn the catalog’s representation above, note the addition of additional elements: long_name, start, end, and dim. Here are the first/last few lines of the enhanced catalog:\n\ndf_enh = coll.df\ndf_enh\n\nWarningThe long_names are not \n\nCF Standard Names, but rather are those documented at \n\nthe NCAR LENS website. For interoperability, the long_name column should be replaced by a cf_name column and possibly an attribute column to disambiguate if needed.\n\nList all available variables by long name, sorted alphabetically:\n\nnameList = coll.unique()['long_name']\nnameList.sort()\nprint(*nameList, sep='\\n')\n\n","type":"content","url":"/notebooks/foundations/enhanced-catalog#long-names","position":15},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Search capabilities"},"type":"lvl2","url":"/notebooks/foundations/enhanced-catalog#search-capabilities","position":16},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Search capabilities"},"content":"\n\nWe can use an intake-esm catalog object’s search function in several ways:\n\nShow all available data for a specific variable based on long name:\n\nmyName = 'Salinity'\ndf = coll.search(long_name=myName).df\ndf\n\nSearch based on multiple criteria:\n\ndf = coll.search(experiment=['20C','RCP85'], dim='3D', variable=['T','Q']).df\ndf\n\n","type":"content","url":"/notebooks/foundations/enhanced-catalog#search-capabilities","position":17},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl3":"Substring matches","lvl2":"Search capabilities"},"type":"lvl3","url":"/notebooks/foundations/enhanced-catalog#substring-matches","position":18},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl3":"Substring matches","lvl2":"Search capabilities"},"content":"\n\nIn some cases, you may not know the exact term to look for. For such cases, inkake-esm supports searching for substring matches. With use of wildcards and/or regular expressions, we can find all items with a particular substring in a given column. Let’s search for:\n\nentries from experiment = ‘20C’\n\nall entries whose variable long name contains wind\n\ncoll_subset = coll.search(experiment=\"20C\", long_name=\"Wind*\")\n\ncoll_subset.df\n\nIf we wanted to search for Wind and wind, we can take advantage of \n\nregular expression syntax to do so:\n\ncoll_subset = coll.search(experiment=\"20C\" , long_name=\"[Ww]ind*\")\n\ncoll_subset.df\n\n","type":"content","url":"/notebooks/foundations/enhanced-catalog#substring-matches","position":19},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl3":"Other attributes","lvl2":"Search capabilities"},"type":"lvl3","url":"/notebooks/foundations/enhanced-catalog#other-attributes","position":20},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl3":"Other attributes","lvl2":"Search capabilities"},"content":"\n\nOther columns in the enhanced catalog may be useful. For example, the dimensionality column enables us to list all data from the ocean component that is 3D.\n\ndf = coll.search(dim=\"3D\",component=\"ocn\").df\ndf\n\n","type":"content","url":"/notebooks/foundations/enhanced-catalog#other-attributes","position":21},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl3":"Spatiotemporal filtering","lvl2":"Search capabilities"},"type":"lvl3","url":"/notebooks/foundations/enhanced-catalog#spatiotemporal-filtering","position":22},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl3":"Spatiotemporal filtering","lvl2":"Search capabilities"},"content":"\n\nIf there were both regional and global data available (e.g., LENS and NA-CORDEX data for the same variable, both listed in same catalog), some type of coverage indicator (or columns for bounding box edges) could be listed.\n\nTemporal extent in LENS is conveyed by the experiment (HIST, 20C, etc) but this is imprecise and requires external documentation. We have added start/end columns to the catalog, but Intake-ESM currently does not have built-in functionality to filter based on time.\n\nWe can do a simple search that exactly matches a temporal value:\n\ndf = coll.search(dim=\"3D\",component=\"ocn\", end='2100-12').df\ndf\n\n\n\n","type":"content","url":"/notebooks/foundations/enhanced-catalog#spatiotemporal-filtering","position":23},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/foundations/enhanced-catalog#summary","position":24},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl2":"Summary"},"content":"In this notebook, we used Intake-ESM to explore a catalog of CESM LENS data. We then worked through some helpful features of the enhanced catalog.","type":"content","url":"/notebooks/foundations/enhanced-catalog#summary","position":25},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/foundations/enhanced-catalog#whats-next","position":26},{"hierarchy":{"lvl1":"Enhanced Intake-ESM Catalog Demo","lvl3":"What’s next?","lvl2":"Summary"},"content":"We will use this data to recreate some figures from a paper published in \n\nBAMS that describes the CESM LENS project: \n\nKay et al. (2015)","type":"content","url":"/notebooks/foundations/enhanced-catalog#whats-next","position":27},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in Project Pythia’s CESM LENS on AWS Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1}]}